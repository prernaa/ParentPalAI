# ParentPalAI - Empathic Fine-Tuning of LLMs using Direct Preference Optimization (DPO) with PEFT (QLoRA)

## Overview
**ParentPalAI** fine-tunes **Mistral Instruct v0.3** on synthetic preference pairs of parenting Q&As generated by GPT-4o. For fine tuning, we use **Direct Preference Optimization (DPO)** and **Parameter Efficient Fine Tuning (PEFT)** with **Quantized Low-Rank Adaptation (QLoRA)**.  
The goal was to explore whether preference alignment using synthetic preference pairs for parenting could make model responses more *empathetic, warm, and relatable* — while retaining clarity and helpfulness.

## Direct Preference Optimization (DPO)

**Base Model:** `mistralai/Mistral-7B-Instruct-v0.3`  
**Fine-tuning Method:** DPO and PEFT (Parameter Efficient Fine Tuning) with QLoRA (Quantized Low-Rank Adaptation)  
**Compute:** Google Colab A100 (4-5 hrs) 
**Number of train samples:** 960
**Number of val samples:** 222
**Number of test samples:** 394
**Epochs:** 1 (best model: `checkpoint-40`, end of epoch 1)

### Training Summary

| Step | Training Loss | Validation Loss | Rewards / Chosen | Rewards / Rejected | Rewards / Accuracies | Rewards / Margins | Logps / Chosen | Logps / Rejected | Logits / Chosen | Logits / Rejected |
|:----:|:--------------:|:----------------:|:----------------:|:------------------:|:--------------------:|:-----------------:|:---------------:|:----------------:|:----------------:|:------------------:|
| 10 | 0.6973 | 0.6738 | 0.8477 | 0.6656 | 0.5721 | 0.1821 | -115.1141 | -109.9999 | -3.3297 | -3.3237 |
| 20 | 0.6567 | 0.7344 | -0.2779 | -0.6143 | 0.6081 | 0.3364 | -126.3695 | -122.7985 | -3.3193 | -3.3136 |
| 30 | 0.6556 | 0.6620 | -0.2510 | -0.5489 | 0.5856 | 0.2978 | -126.1010 | -122.1447 | -3.3344 | -3.3278 |
| 40 | 0.6156 | 0.6545 | -0.2387 | -0.5421 | 0.5991 | 0.3034 | -125.9780 | -122.0773 | -3.3344 | -3.3278 |

**Notes:**  
- Training and validation losses decreased steadily, showing stable DPO fine-tuning convergence.  
- Positive reward margins indicate that chosen responses consistently achieved higher rewards than rejected ones.  
- Log-probabilities (`logps`) and logits remain close, confirming smooth preference optimization without instability.

## Evaluation
ParentPalAI was evaluated using GPT-4o as an LLM-as-a-Judge, comparing its responses (System B) to the base model mistralai/Mistral-7B-Instruct-v0.3 (System A).
Each model pair was scored on six qualitative dimensions — empathy, clarity, comprehensiveness, practicality, adoptability, and overall quality — across 100 GPT-generated parenting prompts.
Two variants of ParentPalAI were tested to understand alignment trade-offs.

We primarily focus on empathy, clarity, and overall quality below, but you can find all results here: https://github.com/prernaa/ParentPalAI/tree/main/dpo_results

### **Version 1 (Empathy-Focused DPO)**  
*(optimized for empathy but considers overall quality)*  

| System | winner_empathy | winner_clarity | winner_overall |
|:-------|:---------------:|:---------------:|:---------------:|
| **System A** | 0.1066 | **0.8883** | **0.7462** |
| **System B (ParentPalAI V1)** | **0.7640** | 0.1117 | 0.2538 |

**Findings:**  
- ParentPalAI V1 dramatically increased *empathy* (+65 points, from ~11% → 76%).  
- The model produced noticeably warmer, more supportive tone but with reduced *clarity* and *practical helpfulness*.  
- Despite lower clarity, some responses were judged as more *relatable* and *emotionally resonant*, showing that empathic alignment can enhance perceived authenticity even when utility drops.

### **Version 2 (Overall-Quality-Focused DPO)**  
*(optimized only for overall win rate)*  

| System | winner_empathy | winner_clarity | winner_overall |
|:-------|:---------------:|:---------------:|:---------------:|
| **System A** | 0.4340 | **0.8604** | **0.6371** |
| **System B (ParentPalAI V2)** | 0.2843 | 0.1371 | 0.3629 |

**Findings:**  
- Optimizing purely for *overall quality* partially recovered clarity and practicality but reduced empathic warmth (43 % → 28 %).  
- The model balanced tone and coherence better than V1 but sounded less emotionally attuned.  
- This highlights a core alignment tension: maximizing clarity and factual strength can come at the expense of empathy and perceived connection.

## Key Learnings
- **V1**: Highest empathy and relatability, weaker clarity -> ideal for exploring affective alignment.  
- **V2**: More balanced but emotionally flatter -> better for generalized instruction following.  
- Empathy and clarity appear inversely correlated when optimizing single-objective DPO.  
- Future work will explore **multi-objective DPO** and **reinforcement from human preferences** to jointly optimize warmth, clarity, and factual helpfulness.

## Datasets
- **Training Data:** `data_to_share/dpo_dataset_dpo_labels_v1.jsonl` and `data_to_share/dpo_dataset_dpo_labels_v2.jsonl`
- **Evaluation Data:** `data_to_share/dpo_dataset_test.jsonl`  
- **Fine-Tuned and Base Model Responses:** `data_to_share/dpo_dataset_test_output_base_dpo_v1.jsonl` and `data_to_share/dpo_dataset_test_output_base_dpo_v2.jsonl` 
- **LLM-as-a-Judge Results:** `data_to_share/dpo_dataset_test_gpt_as_judge_output_v1_new_metrics.jsonl` and `data_to_share/dpo_dataset_test_gpt_as_judge_output_v2.jsonl`

## HuggingFace Model Card
https://huggingface.co/prernac1/parentpalai

## Citation
If you use this work or methodology, please cite as:
```plaintext
@misc{chikersal2025parentpalai,
author = {Prerna Chikersal},
title = {ParentPalAI — Empathic Fine-Tuning of LLMs using Direct Preference Optimization (DPO) with QLoRA},
year = {2025},
publisher = {GitHub},
howpublished = {\url{https://github.com/prernaa/ParentPalAI}},
note = {Hugging Face Model: https://huggingface.co/prernac1/parentpalai}
```



